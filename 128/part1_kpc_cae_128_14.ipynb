{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cda28a8",
   "metadata": {},
   "source": [
    "<h1><center style='color:red'>Information maximization-based clustering of histopathology images (128x128) using deep learning</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7273b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import configparser\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from torchsummary import summary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import RandomAffine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98d9ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device is:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afece543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a stamp to save models\n",
    "\n",
    "stamp = datetime.datetime.now().strftime('%Y%m%d%H%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ada1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom functions for 'marginal entropy', 'conditional entropy' and 'KL divergence'\n",
    "\n",
    "def get_marginal_entropy(x):\n",
    "    me = -torch.sum(x * torch.log(x + 1e-8))\n",
    "    return me\n",
    "\n",
    "def get_conditional_entropy(x):\n",
    "    ce = -torch.sum((x * torch.log(x + 1e-8)), dim=1)\n",
    "    return ce\n",
    "\n",
    "def get_kl_divergence(p, q, batch_mean=True):\n",
    "    kld = torch.sum((p * torch.log(p + 1e-8) - p * torch.log(q + 1e-8)), dim=1)\n",
    "    if (batch_mean):\n",
    "        return (torch.mean(kld))\n",
    "    else:\n",
    "        return kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0367ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom functions for plotting the learning curve\n",
    "\n",
    "class history():\n",
    "    def __init__(self, keys):\n",
    "        self.values = {}\n",
    "        for k in keys:\n",
    "            self.values[k] = []\n",
    "        self.keys = keys\n",
    "        \n",
    "    def append(self, dict_hist):\n",
    "        for k in dict_hist.keys():\n",
    "            self.values[k].append(dict_hist[k])\n",
    "    \n",
    "    def mean(self, keys=None):\n",
    "        if (keys is None):\n",
    "            keys = self.keys\n",
    "        m = {}\n",
    "        for k in keys:\n",
    "            m[k] = np.round(np.mean(self.values[k]), 3)\n",
    "        return m\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return (self.values[key])\n",
    "    \n",
    "    def __str__(self):\n",
    "        get = self.mean(self.keys)\n",
    "        return ('\\t'.join([k + ': ' + str(get[k]) for k in self.keys]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d36fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparameters\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "config['CAE'] = {'rand_seed': 765, 'ks': 3, 'nf0': 15, 'nf1': 45, 'nf2': 128, 'nf3': 196, 'nf4': 128, 'nf5': 128, 'nf6': 128,\n",
    "                 'nfc': 14}\n",
    "\n",
    "config['IM'] = {'lambda_affine': 0.03, 'lambda_marginal_entropy': 0.1, 'lambda_conditional_entropy': 0.03,\n",
    "                'learning_rate': 0.003}\n",
    "\n",
    "config.write(sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf216b",
   "metadata": {},
   "source": [
    "`nfc` denotes the number of clusters. Here, we selected 14 as our chosen number of cluster set. This can be chnaged to 8, 9, 10, 11, 12, 13, 15, 16, 17 and 18 also according to our experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03a87a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = int(config['CAE']['rand_seed'])\n",
    "ks        = int(config['CAE']['ks'])\n",
    "nf0       = int(config['CAE']['nf0'])\n",
    "nf1       = int(config['CAE']['nf1'])\n",
    "nf2       = int(config['CAE']['nf2'])\n",
    "nf3       = int(config['CAE']['nf3'])\n",
    "nf4       = int(config['CAE']['nf4'])\n",
    "nf5       = int(config['CAE']['nf5'])\n",
    "nf6       = int(config['CAE']['nf6'])\n",
    "nfc       = int(config['CAE']['nfc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e57c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_affine              = float(config['IM']['lambda_affine'])\n",
    "lambda_marginal_entropy    = float(config['IM']['lambda_marginal_entropy'])\n",
    "lambda_conditional_entropy = float(config['IM']['lambda_conditional_entropy'])\n",
    "learning_rate              = float(config['IM']['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21230705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "\n",
    "print('random seed:', rand_seed)\n",
    "\n",
    "torch.manual_seed(rand_seed)\n",
    "torch.cuda.manual_seed(rand_seed)\n",
    "np.random.seed(rand_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f15bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "\n",
    "data_src = np.load('/project/dsc-is/mahfujul-r/M/slice128_Block2_20K.npy')\n",
    "print(data_src.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b39039b",
   "metadata": {},
   "source": [
    "`/project/dsc-is/mahfujul-r/M/slice128_Block2_20K.npy` points to the location of dataset that contains randomly created 128x128 patches, where `slice128_Block2_20K.npy` is name of the file of the dataset. From its shape, we can see that it contains 20000 patches; but used 15000 for training the model. 8 indicates the number of stainings, but first, eighth, fiffth, sixth and seventh represent HE, MT, CD31, CK19 and Ki67, respectively and they illustrate identical tissue features. Hence, we used these 5 only. 128x128 is the spatial dimension and 3 depicts RGB channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aff79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the average of RGB values on the channel axis\n",
    "\n",
    "np.round(np.mean(data_src, axis=(0, 1, 2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8087bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom functions to extract batches of samples\n",
    "\n",
    "def get_batch_index_tr(tr, batch_size=None, shuffle=True):\n",
    "    if (shuffle):\n",
    "        np.random.shuffle(tr)\n",
    "    if (batch_size is not None):\n",
    "        n_batch = len(tr) // batch_size\n",
    "    batch_list = np.array_split(tr, n_batch)\n",
    "    return batch_list\n",
    "\n",
    "def get_batch_index_ae(ae, batch_size=None, shuffle=True):\n",
    "    tr = np.arange(ae)\n",
    "    batch_list = get_batch_index_tr(tr, batch_size=batch_size, shuffle=shuffle)\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix, iy = 64, 64 # 128x128 patches are getting rescaled\n",
    "\n",
    "#affine transformation\n",
    "add_random_affine = RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05), fill=(161, 138, 172)) \n",
    "\n",
    "def generate_batch(iii, src, device, random_affine=False):\n",
    "    if (random_affine):\n",
    "        tmp = np.empty((len(iii), ix, iy, nf0))\n",
    "        for aa, ii in enumerate(iii):\n",
    "            img_tmp0 = Image.fromarray(src[ii, 0])\n",
    "            img_tmp0 = add_random_affine(img_tmp0).resize((ix, iy)) # HE\n",
    "            img_tmp4 = Image.fromarray(src[ii, 4])\n",
    "            img_tmp4 = add_random_affine(img_tmp4).resize((ix, iy)) # CD31\n",
    "            img_tmp5 = Image.fromarray(src[ii, 5])\n",
    "            img_tmp5 = add_random_affine(img_tmp5).resize((ix, iy)) # CK19\n",
    "            img_tmp6 = Image.fromarray(src[ii, 6])\n",
    "            img_tmp6 = add_random_affine(img_tmp6).resize((ix, iy)) # Ki67\n",
    "            img_tmp7 = Image.fromarray(src[ii, 7])\n",
    "            img_tmp7 = add_random_affine(img_tmp7).resize((ix, iy)) # MT\n",
    "            tmp[aa] = np.concatenate((img_tmp0, img_tmp4, img_tmp5, img_tmp6, img_tmp7), axis=2)\n",
    "        xxx = torch.tensor(tmp/255.0, dtype=torch.float32).permute(0, 3, 2, 1)\n",
    "    \n",
    "    else:\n",
    "        tmp = np.empty((len(iii), ix, iy, nf0))\n",
    "        for aa, ii in enumerate(iii):\n",
    "            img_tmp0 = Image.fromarray(src[ii, 0]).resize((ix, iy)) # HE\n",
    "            img_tmp4 = Image.fromarray(src[ii, 4]).resize((ix, iy)) # CD31\n",
    "            img_tmp5 = Image.fromarray(src[ii, 5]).resize((ix, iy)) # CK19\n",
    "            img_tmp6 = Image.fromarray(src[ii, 6]).resize((ix, iy)) # Ki67\n",
    "            img_tmp7 = Image.fromarray(src[ii, 7]).resize((ix, iy)) # MT\n",
    "            tmp[aa] = np.concatenate((img_tmp0, img_tmp4, img_tmp5, img_tmp6, img_tmp7), axis=2)\n",
    "        xxx = torch.tensor(tmp/255.0, dtype=torch.float32).permute(0, 3, 2, 1)\n",
    "    \n",
    "    return (xxx.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca13149",
   "metadata": {},
   "source": [
    "Construct `CAE` (Convolutional AutoEncoder) architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4a3cad",
   "metadata": {},
   "source": [
    "- Build an `Encoder` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90793bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential()\n",
    "        self.encoder.add_module('conv1', nn.Conv2d(in_channels=nf0, out_channels=nf1, kernel_size=4, stride=2, padding=1))\n",
    "        self.encoder.add_module('bnor1', nn.BatchNorm2d(num_features=nf1, affine=True, track_running_stats=True))\n",
    "        self.encoder.add_module('lrel1', nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.encoder.add_module('conv2', nn.Conv2d(in_channels=nf1, out_channels=nf2, kernel_size=4, stride=2, padding=1))\n",
    "        self.encoder.add_module('bnor2', nn.BatchNorm2d(num_features=nf2, affine=True, track_running_stats=True))\n",
    "        self.encoder.add_module('lrel2', nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.encoder.add_module('conv3', nn.Conv2d(in_channels=nf2, out_channels=nf3, kernel_size=4, stride=2, padding=1))\n",
    "        self.encoder.add_module('bnor3', nn.BatchNorm2d(num_features=nf3, affine=True, track_running_stats=True))\n",
    "        self.encoder.add_module('lrel3', nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.encoder.add_module('conv4', nn.Conv2d(in_channels=nf3, out_channels=nf4, kernel_size=4, stride=2, padding=1))\n",
    "        self.encoder.add_module('bnor4', nn.BatchNorm2d(num_features=nf4, affine=True, track_running_stats=True))\n",
    "        self.encoder.add_module('lrel4', nn.LeakyReLU(0.1, inplace=True))\n",
    "        \n",
    "    def forward(self, xxx):\n",
    "        hhh = self.encoder(xxx)        \n",
    "        return hhh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252343e5",
   "metadata": {},
   "source": [
    "- Visualize `Encoder` architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfdf889",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(Encoder().to(device), input_size=(nf0, ix, iy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e999b6",
   "metadata": {},
   "source": [
    "- Build a `Classifier` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0ce3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.classifier = nn.Sequential()\n",
    "        self.classifier.add_module('conv1', nn.Conv2d(in_channels=nf4, out_channels=nf5, kernel_size=4, stride=1, padding=0))\n",
    "        self.classifier.add_module('bnor1', nn.BatchNorm2d(num_features=nf5, affine=True, track_running_stats=True))\n",
    "        self.classifier.add_module('lrel1', nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.classifier.add_module('conv2', nn.Conv2d(in_channels=nf5, out_channels=nf6, kernel_size=1, stride=1, padding=0))\n",
    "        self.classifier.add_module('lrel2', nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.classifier.add_module('conv3', nn.Conv2d(in_channels=nf6, out_channels=nfc, kernel_size=1, stride=1, padding=0))\n",
    "        self.classifier.add_module('lrel3', nn.LeakyReLU(0.1, inplace=True))\n",
    "        \n",
    "    def forward(self, hhh):\n",
    "        vvv = self.classifier(hhh)\n",
    "        return vvv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde207eb",
   "metadata": {},
   "source": [
    "- Visualize `Classifier` architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3a3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(Classifier().to(device), input_size=(nf4, ix//16, iy//16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d81b7a",
   "metadata": {},
   "source": [
    "- Build a `Decoder` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2436c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.decoder = nn.Sequential()\n",
    "        self.decoder.add_module('upsm4', nn.UpsamplingBilinear2d(scale_factor=2))\n",
    "        self.decoder.add_module('dcov4', nn.Conv2d(in_channels=nf4 + nfc, out_channels=nf3, kernel_size=3, stride=1, padding=1))\n",
    "        self.decoder.add_module('norm4', nn.BatchNorm2d(num_features=nf3, affine=True, track_running_stats=True))\n",
    "        self.decoder.add_module('lrel4', nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.decoder.add_module('upsm3', nn.UpsamplingBilinear2d(scale_factor=2))\n",
    "        self.decoder.add_module('dcov3', nn.Conv2d(in_channels=nf3, out_channels=nf2, kernel_size=3, stride=1, padding=1))\n",
    "        self.decoder.add_module('norm3', nn.BatchNorm2d(num_features=nf2, affine=True, track_running_stats=True))\n",
    "        self.decoder.add_module('lrel3', nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.decoder.add_module('upsm2', nn.UpsamplingBilinear2d(scale_factor=2))\n",
    "        self.decoder.add_module('dcov2', nn.Conv2d(in_channels=nf2, out_channels=nf1, kernel_size=3, stride=1, padding=1))\n",
    "        self.decoder.add_module('norm2', nn.BatchNorm2d(num_features=nf1, affine=True, track_running_stats=True))\n",
    "        self.decoder.add_module('lrel2', nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.decoder.add_module('upsm1', nn.UpsamplingBilinear2d(scale_factor=2))\n",
    "        self.decoder.add_module('dcov1', nn.Conv2d(in_channels=nf1, out_channels=nf0, kernel_size=3, stride=1, padding=1))\n",
    "        self.decoder.add_module('norm1', nn.BatchNorm2d(num_features=nf0, affine=True, track_running_stats=True))\n",
    "        self.decoder.add_module('sgmd1', nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, hhh, vvv):\n",
    "        ccc = vvv.repeat((1, 1, ix//16, iy//16))\n",
    "        hhh = torch.cat((hhh, ccc), dim=1)\n",
    "        yyy = self.decoder(hhh)\n",
    "        return yyy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2bd427",
   "metadata": {},
   "source": [
    "- Visualize `Decoder` architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c9f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(Decoder().to(device).decoder, input_size=(nf4 + nfc, ix//16, iy//16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133a059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up 'reconstruction loss', 'CAE' and 'optimizer'\n",
    "\n",
    "get_MSELoss = nn.MSELoss(reduction='mean')\n",
    "\n",
    "model_en = Encoder().to(device)\n",
    "model_cl = Classifier().to(device)\n",
    "model_de = Decoder().to(device)\n",
    "\n",
    "optim_en = optim.Adadelta(model_en.parameters(), lr=learning_rate)\n",
    "optim_cl = optim.Adadelta(model_cl.parameters(), lr=learning_rate)\n",
    "optim_de = optim.Adadelta(model_de.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab3719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to train the model\n",
    "\n",
    "t0 = 0\n",
    "key_loss = ['loss', 'loss_rec', 'entropy_marginal', 'entropy_mean', 'loss_affine']\n",
    "loss_hist = history(['tt'] + key_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb19a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_epoch = 4000\n",
    "t_print = 10\n",
    "t_log = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b598f5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training loop for the 'CAE_IM' model\n",
    "\n",
    "for tt in range(t0, t0 + t_epoch):\n",
    "    loss_tt = history(key_loss)\n",
    "    iii_batch = get_batch_index_ae(15000, batch_size=100, shuffle=True) # first 15000 samples to train the model\n",
    "    \n",
    "    for iii in iii_batch:\n",
    "        xxx_tmp = generate_batch(iii, data_src, device)\n",
    "        xxa_tmp = generate_batch(iii, data_src, device, random_affine=True)\n",
    "        \n",
    "        model_en.train()\n",
    "        model_cl.train()\n",
    "        model_de.train()\n",
    "        \n",
    "        hhh_tmp = model_en(xxx_tmp)\n",
    "        vvv_tmp = model_cl(hhh_tmp)\n",
    "        yyy_tmp = model_de(hhh_tmp, vvv_tmp)\n",
    "        \n",
    "        rec_loss = get_MSELoss(xxx_tmp, yyy_tmp)\n",
    "        \n",
    "        ppp_tmp = F.softmax(vvv_tmp.reshape((-1, nfc)), dim=1)\n",
    "        ppp_mean = torch.mean(ppp_tmp, dim=0, keepdim=True)\n",
    "        \n",
    "        marginal_entropy = get_marginal_entropy(ppp_mean)\n",
    "        conditional_entropy = torch.mean(get_conditional_entropy(ppp_tmp))\n",
    "        \n",
    "        hha_tmp = model_en(xxa_tmp)\n",
    "        vva_tmp = model_cl(hha_tmp)\n",
    "        yya_tmp = model_de(hha_tmp, vva_tmp)\n",
    "        \n",
    "        ppa_tmp = F.softmax(vva_tmp.reshape((-1, nfc)), dim=1)\n",
    "        affine_loss = get_kl_divergence(ppp_tmp, ppa_tmp)\n",
    "        \n",
    "        loss_tmp = rec_loss - lambda_marginal_entropy * marginal_entropy + lambda_conditional_entropy * conditional_entropy + \\\n",
    "                   lambda_affine * affine_loss\n",
    "        \n",
    "        optim_en.zero_grad()\n",
    "        optim_cl.zero_grad()\n",
    "        optim_de.zero_grad()\n",
    "        loss_tmp.backward()\n",
    "        optim_en.step()\n",
    "        optim_cl.step()\n",
    "        optim_de.step()\n",
    "        \n",
    "        loss_tt.append({'loss': loss_tmp.item(), 'loss_rec': rec_loss.item(), 'entropy_marginal': marginal_entropy.item(),\n",
    "                        'entropy_mean': conditional_entropy.item(), 'loss_affine': affine_loss.item()})\n",
    "\n",
    "    if (tt + 1) % t_log == 0:\n",
    "        loss_hist.append({'tt': tt})\n",
    "        loss_hist.append(loss_tt.mean())\n",
    "\n",
    "    if (tt + 1) % t_print == 0:\n",
    "        print(tt + 1, '/', t0 + t_epoch, '\\t', str(loss_tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e032a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning curve\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "plt.plot(loss_hist['tt'], loss_hist['loss'], 'r', label='Loss function', linewidth=3)\n",
    "plt.plot(loss_hist['tt'], loss_hist['loss_rec'], 'g', label='Reconstruction loss', linewidth=3)\n",
    "plt.plot(loss_hist['tt'], lambda_marginal_entropy * np.array(loss_hist['entropy_marginal']) - lambda_conditional_entropy * \\\n",
    "         np.array(loss_hist['entropy_mean']), 'b', label='Mutual information (MI)', linewidth=3)\n",
    "\n",
    "plt.xlabel('Number of epochs', fontsize=22)\n",
    "plt.ylabel('Value', fontsize=22)\n",
    "plt.title('The learning curve', fontsize=30)\n",
    "plt.legend(loc='best', fontsize=15, bbox_to_anchor=(0.15, 0.15, 0.755, 1.45));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0254de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check  different shapes\n",
    "\n",
    "print('Encoder input shape\\t:', xxx_tmp.shape)\n",
    "print('\\nEncoder output shape\\t:', hhh_tmp.shape)\n",
    "print('\\nClassifier output shape\\t:', vvv_tmp.shape)\n",
    "print('\\nDecoder output shape\\t:', yyy_tmp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a857266",
   "metadata": {},
   "source": [
    "notice that the encoder input and decoder output shape is similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0533c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_save = 'models14' # folder to save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af9b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_hist = os.path.join(dir_save, f'hist_modelS_{stamp}_{tt + 1}.tsv')\n",
    "print('saving', path_hist) # save training history\n",
    "\n",
    "zzz = pd.DataFrame.from_dict(loss_hist.values)\n",
    "zzz.to_csv(path_hist, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635f7f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models\n",
    "\n",
    "path_model_en = os.path.join(dir_save, f'model_en_{stamp}_{tt + 1}.ckpt')\n",
    "path_model_cl = os.path.join(dir_save, f'model_cl_{stamp}_{tt + 1}.ckpt')\n",
    "path_model_de = os.path.join(dir_save, f'model_de_{stamp}_{tt + 1}.ckpt')\n",
    "\n",
    "print('saving', path_model_en)\n",
    "torch.save(model_en.state_dict(), path_model_en)\n",
    "\n",
    "print('saving', path_model_cl)\n",
    "torch.save(model_cl.state_dict(), path_model_cl)\n",
    "\n",
    "print('saving', path_model_de)\n",
    "torch.save(model_de.state_dict(), path_model_de)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
