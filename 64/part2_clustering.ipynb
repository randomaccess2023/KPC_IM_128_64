{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4212a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import configparser\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import RandomAffine\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88f131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device is:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0455331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparameters\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "config['CAE'] = {'rand_seed': 765, 'ks': 3, 'nf0': 15, 'nf1': 45, 'nf2': 128, 'nf3': 196, 'nf4': 128, 'nf5': 128, 'nf6': 128,\n",
    "                 'nfc': 14}\n",
    "\n",
    "config['IM'] = {'lambda_affine': 0.03, 'lambda_marginal_entropy': 0.1, 'lambda_conditional_entropy': 0.03,\n",
    "                'learning_rate': 0.003}\n",
    "\n",
    "config.write(sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be011c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = int(config['CAE']['rand_seed'])\n",
    "ks        = int(config['CAE']['ks'])\n",
    "nf0       = int(config['CAE']['nf0'])\n",
    "nf1       = int(config['CAE']['nf1'])\n",
    "nf2       = int(config['CAE']['nf2'])\n",
    "nf3       = int(config['CAE']['nf3'])\n",
    "nf4       = int(config['CAE']['nf4'])\n",
    "nf5       = int(config['CAE']['nf5'])\n",
    "nf6       = int(config['CAE']['nf6'])\n",
    "nfc       = int(config['CAE']['nfc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19556432",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_affine              = float(config['IM']['lambda_affine'])\n",
    "lambda_marginal_entropy    = float(config['IM']['lambda_marginal_entropy'])\n",
    "lambda_conditional_entropy = float(config['IM']['lambda_conditional_entropy'])\n",
    "learning_rate              = float(config['IM']['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e316d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "\n",
    "print('random seed:', rand_seed)\n",
    "\n",
    "torch.manual_seed(rand_seed)\n",
    "torch.cuda.manual_seed(rand_seed)\n",
    "np.random.seed(rand_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d188502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "\n",
    "data_src = np.load('/project/dsc-is/mahfujul-r/M/slice64_Block2_20K.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571c8cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom functions to extract batches of samples\n",
    "\n",
    "def get_batch_index_tr(tr, batch_size=None, shuffle=True):\n",
    "    if (shuffle):\n",
    "        np.random.shuffle(tr)\n",
    "    if (batch_size is not None):\n",
    "        n_batch = len(tr) // batch_size\n",
    "    batch_list = np.array_split(tr, n_batch)\n",
    "    return batch_list\n",
    "\n",
    "def get_batch_index_ae(ae, batch_size=None, shuffle=True):\n",
    "    tr = np.arange(ae)\n",
    "    batch_list = get_batch_index_tr(tr, batch_size=batch_size, shuffle=shuffle)\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89a6f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix, iy = 64, 64 # 64x64 patches are not getting rescaled like 128x128 patches\n",
    "\n",
    "#affine transformation\n",
    "add_random_affine = RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05), fill=(161, 138, 172)) \n",
    "\n",
    "def generate_batch(iii, src, device, random_affine=False):\n",
    "    if (random_affine):\n",
    "        tmp = np.empty((len(iii), ix, iy, nf0))\n",
    "        for aa, ii in enumerate(iii):\n",
    "            img_tmp0 = Image.fromarray(src[ii, 0])\n",
    "            img_tmp0 = add_random_affine(img_tmp0) # HE\n",
    "            img_tmp4 = Image.fromarray(src[ii, 4])\n",
    "            img_tmp4 = add_random_affine(img_tmp4) # CD31\n",
    "            img_tmp5 = Image.fromarray(src[ii, 5])\n",
    "            img_tmp5 = add_random_affine(img_tmp5) # CK19\n",
    "            img_tmp6 = Image.fromarray(src[ii, 6])\n",
    "            img_tmp6 = add_random_affine(img_tmp6) # Ki67\n",
    "            img_tmp7 = Image.fromarray(src[ii, 7])\n",
    "            img_tmp7 = add_random_affine(img_tmp7) # MT\n",
    "            tmp[aa] = np.concatenate((img_tmp0, img_tmp4, img_tmp5, img_tmp6, img_tmp7), axis=2)\n",
    "        xxx = torch.tensor(tmp/255.0, dtype=torch.float32).permute(0, 3, 2, 1)\n",
    "    \n",
    "    else:\n",
    "        tmp = np.empty((len(iii), ix, iy, nf0))\n",
    "        for aa, ii in enumerate(iii):\n",
    "            img_tmp0 = Image.fromarray(src[ii, 0]) # HE\n",
    "            img_tmp4 = Image.fromarray(src[ii, 4]) # CD31\n",
    "            img_tmp5 = Image.fromarray(src[ii, 5]) # CK19\n",
    "            img_tmp6 = Image.fromarray(src[ii, 6]) # Ki67\n",
    "            img_tmp7 = Image.fromarray(src[ii, 7]) # MT\n",
    "            tmp[aa] = np.concatenate((img_tmp0, img_tmp4, img_tmp5, img_tmp6, img_tmp7), axis=2)\n",
    "        xxx = torch.tensor(tmp/255.0, dtype=torch.float32).permute(0, 3, 2, 1)\n",
    "    \n",
    "    return (xxx.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76ec47",
   "metadata": {},
   "source": [
    "`CAE` (Convolutional AutoEncoder) architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816fc8c9",
   "metadata": {},
   "source": [
    "- `Encoder` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224babe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential()\n",
    "        self.encoder.add_module('conv1', nn.Conv2d(in_channels=nf0, out_channels=nf1, kernel_size=4, stride=2, padding=1))\n",
    "        self.encoder.add_module('bnor1', nn.BatchNorm2d(num_features=nf1, affine=True, track_running_stats=True))\n",
    "        self.encoder.add_module('lrel1', nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.encoder.add_module('conv2', nn.Conv2d(in_channels=nf1, out_channels=nf2, kernel_size=4, stride=2, padding=1))\n",
    "        self.encoder.add_module('bnor2', nn.BatchNorm2d(num_features=nf2, affine=True, track_running_stats=True))\n",
    "        self.encoder.add_module('lrel2', nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.encoder.add_module('conv3', nn.Conv2d(in_channels=nf2, out_channels=nf3, kernel_size=4, stride=2, padding=1))\n",
    "        self.encoder.add_module('bnor3', nn.BatchNorm2d(num_features=nf3, affine=True, track_running_stats=True))\n",
    "        self.encoder.add_module('lrel3', nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.encoder.add_module('conv4', nn.Conv2d(in_channels=nf3, out_channels=nf4, kernel_size=4, stride=2, padding=1))\n",
    "        self.encoder.add_module('bnor4', nn.BatchNorm2d(num_features=nf4, affine=True, track_running_stats=True))\n",
    "        self.encoder.add_module('lrel4', nn.LeakyReLU(0.1, inplace=True))\n",
    "        \n",
    "    def forward(self, xxx):\n",
    "        hhh = self.encoder(xxx)        \n",
    "        return hhh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297582b8",
   "metadata": {},
   "source": [
    "- `Classifier` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.classifier = nn.Sequential()\n",
    "        self.classifier.add_module('conv1', nn.Conv2d(in_channels=nf4, out_channels=nf5, kernel_size=4, stride=1, padding=0))\n",
    "        self.classifier.add_module('bnor1', nn.BatchNorm2d(num_features=nf5, affine=True, track_running_stats=True))\n",
    "        self.classifier.add_module('lrel1', nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.classifier.add_module('conv2', nn.Conv2d(in_channels=nf5, out_channels=nf6, kernel_size=1, stride=1, padding=0))\n",
    "        self.classifier.add_module('lrel2', nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.classifier.add_module('conv3', nn.Conv2d(in_channels=nf6, out_channels=nfc, kernel_size=1, stride=1, padding=0))\n",
    "        self.classifier.add_module('lrel3', nn.LeakyReLU(0.1, inplace=True))\n",
    "        \n",
    "    def forward(self, hhh):\n",
    "        vvv = self.classifier(hhh)\n",
    "        return vvv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6c260c",
   "metadata": {},
   "source": [
    "- `Decoder` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d078d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.decoder = nn.Sequential()\n",
    "        self.decoder.add_module('upsm4', nn.UpsamplingBilinear2d(scale_factor=2))\n",
    "        self.decoder.add_module('dcov4', nn.Conv2d(in_channels=nf4 + nfc, out_channels=nf3, kernel_size=3, stride=1, padding=1))\n",
    "        self.decoder.add_module('norm4', nn.BatchNorm2d(num_features=nf3, affine=True, track_running_stats=True))\n",
    "        self.decoder.add_module('lrel4', nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.decoder.add_module('upsm3', nn.UpsamplingBilinear2d(scale_factor=2))\n",
    "        self.decoder.add_module('dcov3', nn.Conv2d(in_channels=nf3, out_channels=nf2, kernel_size=3, stride=1, padding=1))\n",
    "        self.decoder.add_module('norm3', nn.BatchNorm2d(num_features=nf2, affine=True, track_running_stats=True))\n",
    "        self.decoder.add_module('lrel3', nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.decoder.add_module('upsm2', nn.UpsamplingBilinear2d(scale_factor=2))\n",
    "        self.decoder.add_module('dcov2', nn.Conv2d(in_channels=nf2, out_channels=nf1, kernel_size=3, stride=1, padding=1))\n",
    "        self.decoder.add_module('norm2', nn.BatchNorm2d(num_features=nf1, affine=True, track_running_stats=True))\n",
    "        self.decoder.add_module('lrel2', nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.decoder.add_module('upsm1', nn.UpsamplingBilinear2d(scale_factor=2))\n",
    "        self.decoder.add_module('dcov1', nn.Conv2d(in_channels=nf1, out_channels=nf0, kernel_size=3, stride=1, padding=1))\n",
    "        self.decoder.add_module('norm1', nn.BatchNorm2d(num_features=nf0, affine=True, track_running_stats=True))\n",
    "        self.decoder.add_module('sgmd1', nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, hhh, vvv):\n",
    "        ccc = vvv.repeat((1, 1, ix//16, iy//16))\n",
    "        hhh = torch.cat((hhh, ccc), dim=1)\n",
    "        yyy = self.decoder(hhh)\n",
    "        return yyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_en = Encoder().to(device)\n",
    "model_cl = Classifier().to(device)\n",
    "model_de = Decoder().to(device)\n",
    "\n",
    "# load models\n",
    "model_en.load_state_dict(torch.load('models14/model_encoder_3000'))\n",
    "model_cl.load_state_dict(torch.load('models14/model_classifier_3000'))\n",
    "model_de.load_state_dict(torch.load('models14/model_decoder_3000'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iii_batch = get_batch_index_ae(15000, batch_size=100, shuffle=False) # first 15000 samples\n",
    "\n",
    "hhh_list = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for iii in iii_batch:\n",
    "        xxx_tmp = generate_batch(iii, data_src, device)\n",
    "        \n",
    "        model_en.eval()\n",
    "        \n",
    "        hhh_tmp = model_en(xxx_tmp)\n",
    "        \n",
    "        hhh_list.append(torch.softmax(hhh_tmp, dim=1).detach().cpu().numpy())\n",
    "        \n",
    "    hhh = np.concatenate(hhh_list, axis=0)\n",
    "    print('Shape of upper latent space:', hhh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07ca256",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.mean(hhh, axis=3)\n",
    "a = np.mean(a, axis=2)\n",
    "a.shape # embedded features in upper latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b939c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "iii_batch = get_batch_index_ae(15000, batch_size=100, shuffle=False) # first 15000 samples\n",
    "\n",
    "vvv_list = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for iii in iii_batch:\n",
    "        xxx_tmp = generate_batch(iii, data_src, device)\n",
    "        \n",
    "        model_en.eval()\n",
    "        model_cl.eval()\n",
    "        \n",
    "        hhh_tmp = model_en(xxx_tmp)\n",
    "        vvv_tmp = model_cl(hhh_tmp)\n",
    "        \n",
    "        vvv_list.append(torch.softmax(vvv_tmp.reshape((-1, nfc)), dim=1).detach().cpu().numpy())\n",
    "        \n",
    "    vvv_15k = np.concatenate(vvv_list, axis=0)\n",
    "    print('Embedded features in lower latent space:', vvv_15k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45989f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = []\n",
    "D = []\n",
    "\n",
    "for j in range(len(vvv_15k)):\n",
    "    hemp1 = vvv_15k[j, :]\n",
    "    hemp2 = np.argmax(hemp1)\n",
    "    hemp3 = hemp1[hemp2]\n",
    "    C.append(hemp2) # collect cluster IDs with highest probability for each patch\n",
    "    D.append(hemp3) # collect highest probability of each patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a131e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.DataFrame(a)\n",
    "DF['128'] = C\n",
    "DF['129'] = D\n",
    "DF # this dataframe contains embedded features from upper latent space, cluster IDs and highest probability (of each patch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f468cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'HHH14 & C14 & D14.csv' file will be saved in the present working directory\n",
    "\n",
    "DF.to_csv('HHH14 & C14 & D14.csv', index=None, header=None) # save DF as a '.csv' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941dfaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering\n",
    "\n",
    "fig, plts = plt.subplots(nrows=14, ncols=1)\n",
    "\n",
    "for cc in range(nfc):\n",
    "    iii = np.argsort(-vvv_15k[:, cc])\n",
    "    \n",
    "    tmp_p1 = []\n",
    "    tmp_p2 = []\n",
    "    tmp_p3 = []\n",
    "    tmp_p4 = []\n",
    "    tmp_p5 = []\n",
    "    \n",
    "    for aa in range(8):\n",
    "        tmp_p1.append(torch.from_numpy(data_src[iii[aa], 0]).permute(2, 0, 1))\n",
    "        tmp_p2.append(torch.from_numpy(data_src[iii[aa], 7]).permute(2, 0, 1))\n",
    "        tmp_p3.append(torch.from_numpy(data_src[iii[aa], 4]).permute(2, 0, 1))\n",
    "        tmp_p4.append(torch.from_numpy(data_src[iii[aa], 5]).permute(2, 0, 1))\n",
    "        tmp_p5.append(torch.from_numpy(data_src[iii[aa], 6]).permute(2, 0, 1))\n",
    "    \n",
    "    grid = make_grid([tmp_p1[0], tmp_p1[1], tmp_p1[2], tmp_p1[3], tmp_p1[4], tmp_p1[5], tmp_p1[6], tmp_p1[7],\n",
    "                      tmp_p2[0], tmp_p2[1], tmp_p2[2], tmp_p2[3], tmp_p2[4], tmp_p2[5], tmp_p2[6], tmp_p2[7],\n",
    "                      tmp_p3[0], tmp_p3[1], tmp_p3[2], tmp_p3[3], tmp_p3[4], tmp_p3[5], tmp_p3[6], tmp_p3[7],\n",
    "                      tmp_p4[0], tmp_p4[1], tmp_p4[2], tmp_p4[3], tmp_p4[4], tmp_p4[5], tmp_p4[6], tmp_p4[7],\n",
    "                      tmp_p5[0], tmp_p5[1], tmp_p5[2], tmp_p5[3], tmp_p5[4], tmp_p5[5], tmp_p5[6], tmp_p5[7]],\n",
    "                      nrow=8, padding=3)\n",
    "    \n",
    "    img = torchvision.transforms.ToPILImage()(grid)\n",
    "    plts[cc].imshow(img)\n",
    "    plts[cc].set_ylabel('Cluster {}'.format(cc + 1), fontsize=12)\n",
    "    plts[cc].set_xticks([])\n",
    "    plts[cc].set_yticks([])\n",
    "\n",
    "fig.set_size_inches((10, 30))\n",
    "fig.set_dpi(300)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
